/**
 * File has been automatically generated by `typo`.
 *
 * IF YOU CHANGE THIS FILE YOUR CHANGES WILL BE OVERWRITTEN.
 */
package adventureworks.humanresources.shift

import java.sql.Connection
import java.util.ArrayList
import java.util.HashMap
import java.util.Optional
import java.util.function.Function
import java.util.stream.Collectors
import typo.dsl.DeleteBuilder
import typo.dsl.DeleteBuilder.DeleteBuilderMock
import typo.dsl.DeleteParams
import typo.dsl.SelectBuilder
import typo.dsl.SelectBuilderMock
import typo.dsl.SelectParams
import typo.dsl.UpdateBuilder
import typo.dsl.UpdateBuilder.UpdateBuilderMock
import typo.dsl.UpdateParams

case class ShiftRepoMock(
  toRow: ShiftRowUnsaved => ShiftRow,
  map: HashMap[ShiftId, ShiftRow] = new HashMap[ShiftId, ShiftRow]()
) extends ShiftRepo {
  def delete: DeleteBuilder[ShiftFields, ShiftRow] = {
    new DeleteBuilderMock(
      ShiftFields.structure,
      () => new ArrayList(map.values()),
      DeleteParams.empty(),
      row => row.shiftid,
      id => map.remove(id): @scala.annotation.nowarn
    )
  }

  def deleteById(shiftid: ShiftId)(using c: Connection): java.lang.Boolean = Optional.ofNullable(map.remove(shiftid)).isPresent()

  def deleteByIds(shiftids: Array[ShiftId])(using c: Connection): Integer = {
    var count = 0
    shiftids.foreach { id => if (Optional.ofNullable(map.remove(id)).isPresent()) {
      count = count + 1
    } }
    count
  }

  def insert(unsaved: ShiftRow)(using c: Connection): ShiftRow = {
    if (map.containsKey(unsaved.shiftid)) {
      throw new RuntimeException(s"id $unsaved.shiftid already exists")
    }
    map.put(unsaved.shiftid, unsaved): @scala.annotation.nowarn
    unsaved
  }

  def insert(unsaved: ShiftRowUnsaved)(using c: Connection): ShiftRow = insert(toRow(unsaved))(using c)

  def insertStreaming(
    unsaved: java.util.Iterator[ShiftRow],
    batchSize: Integer = 10000
  )(using c: Connection): java.lang.Long = {
    var count = 0L
    while (unsaved.hasNext()) {
      val row = unsaved.next()
      map.put(row.shiftid, row): @scala.annotation.nowarn
      count = count + 1L
    }
    count
  }

  /** NOTE: this functionality requires PostgreSQL 16 or later! */
  def insertUnsavedStreaming(
    unsaved: java.util.Iterator[ShiftRowUnsaved],
    batchSize: Integer = 10000
  )(using c: Connection): java.lang.Long = {
    var count = 0L
    while (unsaved.hasNext()) {
      val unsavedRow = unsaved.next()
      val row = toRow(unsavedRow)
      map.put(row.shiftid, row): @scala.annotation.nowarn
      count = count + 1L
    }
    count
  }

  def select: SelectBuilder[ShiftFields, ShiftRow] = new SelectBuilderMock(ShiftFields.structure, () => new ArrayList(map.values()), SelectParams.empty())

  def selectAll(using c: Connection): java.util.List[ShiftRow] = new ArrayList(map.values())

  def selectById(shiftid: ShiftId)(using c: Connection): Optional[ShiftRow] = Optional.ofNullable(map.get(shiftid))

  def selectByIds(shiftids: Array[ShiftId])(using c: Connection): java.util.List[ShiftRow] = {
    val result = new ArrayList[ShiftRow]()
    shiftids.foreach { id => val opt = Optional.ofNullable(map.get(id))
    if (opt.isPresent()) result.add(opt.get()): @scala.annotation.nowarn }
    result
  }

  def selectByIdsTracked(shiftids: Array[ShiftId])(using c: Connection): java.util.Map[ShiftId, ShiftRow] = selectByIds(shiftids)(using c).stream().collect(Collectors.toMap((row: adventureworks.humanresources.shift.ShiftRow) => row.shiftid, Function.identity()))

  def update: UpdateBuilder[ShiftFields, ShiftRow] = {
    new UpdateBuilderMock(
      ShiftFields.structure,
      () => new ArrayList(map.values()),
      UpdateParams.empty(),
      row => row
    )
  }

  def update(row: ShiftRow)(using c: Connection): java.lang.Boolean = {
    val shouldUpdate = Optional.ofNullable(map.get(row.shiftid)).filter(oldRow => !oldRow.equals(row)).isPresent()
    if (shouldUpdate) {
      map.put(row.shiftid, row): @scala.annotation.nowarn
    }
    shouldUpdate
  }

  def upsert(unsaved: ShiftRow)(using c: Connection): ShiftRow = {
    map.put(unsaved.shiftid, unsaved): @scala.annotation.nowarn
    unsaved
  }

  def upsertBatch(unsaved: java.util.Iterator[ShiftRow])(using c: Connection): java.util.List[ShiftRow] = {
    val result = new ArrayList[ShiftRow]()
    while (unsaved.hasNext()) {
      val row = unsaved.next()
      map.put(row.shiftid, row): @scala.annotation.nowarn
      result.add(row): @scala.annotation.nowarn
    }
    result
  }

  /** NOTE: this functionality is not safe if you use auto-commit mode! it runs 3 SQL statements */
  def upsertStreaming(
    unsaved: java.util.Iterator[ShiftRow],
    batchSize: Integer = 10000
  )(using c: Connection): Integer = {
    var count = 0
    while (unsaved.hasNext()) {
      val row = unsaved.next()
      map.put(row.shiftid, row): @scala.annotation.nowarn
      count = count + 1
    }
    count
  }
}