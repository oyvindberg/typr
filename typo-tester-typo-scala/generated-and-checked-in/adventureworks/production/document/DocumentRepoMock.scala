/**
 * File has been automatically generated by `typo`.
 *
 * IF YOU CHANGE THIS FILE YOUR CHANGES WILL BE OVERWRITTEN.
 */
package adventureworks.production.document

import adventureworks.customtypes.TypoUUID
import java.sql.Connection
import java.util.ArrayList
import java.util.HashMap
import java.util.Optional
import java.util.function.Function
import java.util.stream.Collectors
import typo.dsl.DeleteBuilder
import typo.dsl.DeleteBuilder.DeleteBuilderMock
import typo.dsl.DeleteParams
import typo.dsl.SelectBuilder
import typo.dsl.SelectBuilderMock
import typo.dsl.SelectParams
import typo.dsl.UpdateBuilder
import typo.dsl.UpdateBuilder.UpdateBuilderMock
import typo.dsl.UpdateParams

case class DocumentRepoMock(
  toRow: DocumentRowUnsaved => DocumentRow,
  map: HashMap[DocumentId, DocumentRow] = new HashMap[DocumentId, DocumentRow]()
) extends DocumentRepo {
  def delete: DeleteBuilder[DocumentFields, DocumentRow] = {
    new DeleteBuilderMock(
      DocumentFields.structure,
      () => new ArrayList(map.values()),
      DeleteParams.empty(),
      row => row.documentnode,
      id => map.remove(id): @scala.annotation.nowarn
    )
  }

  def deleteById(documentnode: DocumentId)(using c: Connection): java.lang.Boolean = Optional.ofNullable(map.remove(documentnode)).isPresent()

  def deleteByIds(documentnodes: Array[DocumentId])(using c: Connection): Integer = {
    var count = 0
    documentnodes.foreach { id => if (Optional.ofNullable(map.remove(id)).isPresent()) {
      count = count + 1
    } }
    count
  }

  def insert(unsaved: DocumentRow)(using c: Connection): DocumentRow = {
    if (map.containsKey(unsaved.documentnode)) {
      throw new RuntimeException(s"id $unsaved.documentnode already exists")
    }
    map.put(unsaved.documentnode, unsaved): @scala.annotation.nowarn
    unsaved
  }

  def insert(unsaved: DocumentRowUnsaved)(using c: Connection): DocumentRow = insert(toRow(unsaved))(using c)

  def insertStreaming(
    unsaved: java.util.Iterator[DocumentRow],
    batchSize: Integer = 10000
  )(using c: Connection): java.lang.Long = {
    var count = 0L
    while (unsaved.hasNext()) {
      val row = unsaved.next()
      map.put(row.documentnode, row): @scala.annotation.nowarn
      count = count + 1L
    }
    count
  }

  /** NOTE: this functionality requires PostgreSQL 16 or later! */
  def insertUnsavedStreaming(
    unsaved: java.util.Iterator[DocumentRowUnsaved],
    batchSize: Integer = 10000
  )(using c: Connection): java.lang.Long = {
    var count = 0L
    while (unsaved.hasNext()) {
      val unsavedRow = unsaved.next()
      val row = toRow(unsavedRow)
      map.put(row.documentnode, row): @scala.annotation.nowarn
      count = count + 1L
    }
    count
  }

  def select: SelectBuilder[DocumentFields, DocumentRow] = new SelectBuilderMock(DocumentFields.structure, () => new ArrayList(map.values()), SelectParams.empty())

  def selectAll(using c: Connection): java.util.List[DocumentRow] = new ArrayList(map.values())

  def selectById(documentnode: DocumentId)(using c: Connection): Optional[DocumentRow] = Optional.ofNullable(map.get(documentnode))

  def selectByIds(documentnodes: Array[DocumentId])(using c: Connection): java.util.List[DocumentRow] = {
    val result = new ArrayList[DocumentRow]()
    documentnodes.foreach { id => val opt = Optional.ofNullable(map.get(id))
    if (opt.isPresent()) result.add(opt.get()): @scala.annotation.nowarn }
    result
  }

  def selectByIdsTracked(documentnodes: Array[DocumentId])(using c: Connection): java.util.Map[DocumentId, DocumentRow] = selectByIds(documentnodes)(using c).stream().collect(Collectors.toMap((row: adventureworks.production.document.DocumentRow) => row.documentnode, Function.identity()))

  def selectByUniqueRowguid(rowguid: TypoUUID)(using c: Connection): Optional[DocumentRow] = new ArrayList(map.values()).stream().filter(v => rowguid.equals(v.rowguid)).findFirst()

  def update: UpdateBuilder[DocumentFields, DocumentRow] = {
    new UpdateBuilderMock(
      DocumentFields.structure,
      () => new ArrayList(map.values()),
      UpdateParams.empty(),
      row => row
    )
  }

  def update(row: DocumentRow)(using c: Connection): java.lang.Boolean = {
    val shouldUpdate = Optional.ofNullable(map.get(row.documentnode)).filter(oldRow => !oldRow.equals(row)).isPresent()
    if (shouldUpdate) {
      map.put(row.documentnode, row): @scala.annotation.nowarn
    }
    shouldUpdate
  }

  def upsert(unsaved: DocumentRow)(using c: Connection): DocumentRow = {
    map.put(unsaved.documentnode, unsaved): @scala.annotation.nowarn
    unsaved
  }

  def upsertBatch(unsaved: java.util.Iterator[DocumentRow])(using c: Connection): java.util.List[DocumentRow] = {
    val result = new ArrayList[DocumentRow]()
    while (unsaved.hasNext()) {
      val row = unsaved.next()
      map.put(row.documentnode, row): @scala.annotation.nowarn
      result.add(row): @scala.annotation.nowarn
    }
    result
  }

  /** NOTE: this functionality is not safe if you use auto-commit mode! it runs 3 SQL statements */
  def upsertStreaming(
    unsaved: java.util.Iterator[DocumentRow],
    batchSize: Integer = 10000
  )(using c: Connection): Integer = {
    var count = 0
    while (unsaved.hasNext()) {
      val row = unsaved.next()
      map.put(row.documentnode, row): @scala.annotation.nowarn
      count = count + 1
    }
    count
  }
}